{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Prepared Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the data cleaning and chosen vocabulary to prepare each movie review and save the prepared version of the reviews ready for modelling. This is a good practise as it decouples the data preparation from modeling, allowing you to focus on modelling and circle back to data prep if ypu have new ideas.\n",
    "We can start off by loading the vocabulary from vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load doc into memory\n",
    "def load_doc(filename):\n",
    "    #open the file as read only\n",
    "    file=open(filename,'r')\n",
    "    #read all text\n",
    "    text=file.read()\n",
    "    #close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NExt, we can clean the reviews, use the loaded vocab to filter out unwanted tokens, and save the \n",
    "clean reviews in a file. Our approach could be to save all the positive reviews in one file and all the negative reviews in another file, with the filtered tokens separated by white spce for each review on separate lines.\n",
    "First, we can deﬁne a function to process a document, clean it, ﬁlter it, and return it as a single line that could be saved in a ﬁle. Below deﬁnes the doc to line() function to do just that, taking a ﬁlename and vocabulary (as a set) as arguments. It calls the previously deﬁned load doc() function to load the document and clean doc() to tokenize the document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    #split into tokens by white space\n",
    "    tokens=doc.split()\n",
    "    #prepare a regex for char filtering\n",
    "    re_punc=re.compile('[%s]' %re.escape(string.punctuation))\n",
    "    #remove punctuation from each word\n",
    "    tokens=[re_punc.sub('',w) for w in tokens]\n",
    "    #remove remaining tokens that are not alphabetic\n",
    "    tokens=[word for word in tokens if word.isalpha()]\n",
    "    #filter out stopwords\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    tokens=[w for w in tokens if not w in stop_words]\n",
    "    #filter out short tokens\n",
    "    tokens=[word for word in tokens if len(word)>1]\n",
    "    return tokens    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save list to file\n",
    "def save_list(lines,filename):\n",
    "    data='\\n'.join(lines)\n",
    "    file=open(filename,'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load doc,clean and return lines of tokens\n",
    "def doc_to_line(filename,vocab):\n",
    "    doc=load_doc(filename)\n",
    "    tokens=clean_doc(doc)\n",
    "    #filter by vocab\n",
    "    tokens=[w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can deﬁne a new version of process docs() to step through all reviews in a folder and convert them to lines by calling doc to line() for each document. A list of lines is then returned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load docs in a directory\n",
    "def process_docs(directory,vocab):\n",
    "    lines=list()\n",
    "    #walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        if not filename in listdir(directory):\n",
    "            next\n",
    "        #create the full path of the file to open\n",
    "        path=directory+'/'+filename\n",
    "        #load and clean the doc\n",
    "        line=doc_to_line(path,vocab)\n",
    "        #add to list\n",
    "        lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_filename='vocab.txt'\n",
    "vocab=load_doc(vocab_filename)\n",
    "vocab=vocab.split()\n",
    "vocab=set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare negative reviews\n",
    "negative_lines=process_docs('data/review_polarity/txt_sentoken/neg',vocab)\n",
    "save_list(negative_lines,'negative.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare positive reviews\n",
    "positive_lines=process_docs('data/review_polarity/txt_sentoken/pos',vocab)\n",
    "save_list(positive_lines,'positive.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the examples saves two new files, negative.txt and positive.txt, that conatain the prepared negative and positive reviews respectively.\n",
    "The data is ready for use in a bag-of-words or even word embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
