{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load doc into memory\n",
    "def load_doc(filename):\n",
    "    #open the file as read only\n",
    "    file=open(filename,\"r\")\n",
    "    #read all text\n",
    "    text=file.read()\n",
    "    #close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    #split tokens into white spaces\n",
    "    tokens=doc.split()\n",
    "    #prepare regex for char filtering\n",
    "    re_punc=re.compile('[%s]'%re.escape(string.punctuation))\n",
    "    #remove punctuation from each word\n",
    "    tokens=[re_punc.sub('',w) for w in tokens]\n",
    "    #remove remaining tokens that are not alphabetic\n",
    "    tokens=[word for word in tokens if word.isalpha()]\n",
    "    #filter out stopwords\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    tokens=[word for word in tokens if not word in stop_words]\n",
    "    #filter out short tokens\n",
    "    tokens=[word for word in tokens if len(word)>1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load doc and add to vocab\n",
    "def add_doc_to_vocab(filename,vocab):\n",
    "    #load doc\n",
    "    doc=load_doc(filename)\n",
    "    #clean doc\n",
    "    tokens=clean_doc(doc)\n",
    "    #update counts\n",
    "    vocab.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all docs ina directory\n",
    "def process_docs(directory,vocab):\n",
    "    #walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        #skip files that do not have the right extension\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            next\n",
    "        #create the full path\n",
    "        path=directory+'/'+filename\n",
    "        add_doc_to_vocab(path,vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reduces the vocab from 46,557 to 14,803 words, a huge drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list(lines,filename):\n",
    "    data='\\n'.join(lines)\n",
    "    file=open(filename,'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46557\n"
     ]
    }
   ],
   "source": [
    "vocab=Counter()\n",
    "directory1=\"data/review_polarity/txt_sentoken/neg\"\n",
    "directory2=\"data/review_polarity/txt_sentoken/pos\"\n",
    "process_docs(directory1,vocab)\n",
    "process_docs(directory2,vocab)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example creates a vocabulary witha ll documents in the data set , including positive and negative reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('film', 8860), ('one', 5521), ('movie', 5440), ('like', 3553), ('even', 2555), ('good', 2320), ('time', 2283), ('story', 2118), ('films', 2102), ('would', 2042), ('much', 2024), ('also', 1965), ('characters', 1947), ('get', 1921), ('character', 1906), ('two', 1825), ('first', 1768), ('see', 1730), ('well', 1694), ('way', 1668), ('make', 1590), ('really', 1563), ('little', 1491), ('life', 1472), ('plot', 1451), ('people', 1420), ('movies', 1416), ('could', 1395), ('bad', 1374), ('scene', 1373), ('never', 1364), ('best', 1301), ('new', 1277), ('many', 1268), ('doesnt', 1267), ('man', 1266), ('scenes', 1265), ('dont', 1210), ('know', 1207), ('hes', 1150), ('great', 1141), ('another', 1111), ('love', 1089), ('action', 1078), ('go', 1075), ('us', 1065), ('director', 1056), ('something', 1048), ('end', 1047), ('still', 1038)]\n"
     ]
    }
   ],
   "source": [
    "print(vocab.most_common(50))\n",
    "#most common are film,one,movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps the least common words, those that only appear once across all reviews are not predictive. Perhaps some of the most common words are not useful too. These are good questions and really should be tested with a speciﬁc predictive model. Generally, words that only appear once or a few times across 2,000 reviews are probably not predictive and can be removed from the vocabulary, greatly cutting down on the tokens we need to model. We can do this by stepping through words and their counts and only keeping those with a count above a chosen threshold. Here we will use 5 occurrences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14803\n"
     ]
    }
   ],
   "source": [
    "#keep tokens with >5 occurrence\n",
    "min_occurrence=5\n",
    "tokens=[k for k,c in vocab.items() if c >=min_occurrence]\n",
    "print(len(tokens))\n",
    "save_list(tokens,\"vocab.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this final snippet after creating the vocabulary will save the chosen words to file. It is a giid idea to take a look at, and even study, your chosen vocabulary in order to get ideas for better preparing this data, or text data in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can look at using the vocabulary to create a prepared version of the movie review dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
